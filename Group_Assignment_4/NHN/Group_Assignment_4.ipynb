{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO3hmhPw3g+s7GVWLQ/ugv9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Korsholm22/M4_Group_Assignments/blob/main/Group_Assignment_4/NHN/Group_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task"
      ],
      "metadata": {
        "id": "XvRDLJyvEESW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build an exctiting and perhapps also fun application using techniques learned in this module Submission: Github repo as usual.\n",
        "\n",
        "**Minimal requirements:**\n",
        "- Relevant task solved\n",
        "- Self-trained or fine-tuned transformer, however not sentence transformer for semantic search only (you are welcome to explore techniques beyond the scope of the course e.g. on HF)\n",
        "published on HF\n",
        "- Gradio (in-notebook) app or HF spaces\n",
        "\n",
        "\n",
        "**Nice-to:**\n",
        "- Streamlit app on Hub\n",
        "- Optional use of API (HF Inference API, Cohere, be careful with OpenAI ðŸ’¸)\n",
        "- Optional more complex LLM setup with e.g. langchain, promptify, pinecone and other integrations etc."
      ],
      "metadata": {
        "id": "D3a31eShEDh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning a Whisper Model for Automatic Speech Recognition"
      ],
      "metadata": {
        "id": "AdF_KC9gEQBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following a tutorial by Sanchit Gandhi, we have fine-tuned a Whisper Model based on the Common Voice dataset from Mozilla using Hugging Face ðŸ¤— Transformers. Whisper is a pre-trained model for automatic speech recognition (ASR) published in 2022, and it is trained on 680,000 hours of labelled audio-transciption data.\n",
        "\n",
        "The model is currently available in five different sizes, which vary in numbers of layers, parameters, languages and so forth.\n",
        "\n",
        "Due to time constraints and computational power available on Google Colab, we will work with the smallest model called \"tiny\" in the following."
      ],
      "metadata": {
        "id": "G5Q_BDPwFDkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "QebVI83RIQZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we verify whether we have access to GPU on Google Colab or not\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "Y_lB3PqcEDTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv87ctd8D38e"
      },
      "outputs": [],
      "source": [
        "# Pip installing necessary packages\n",
        "!pip install datasets>=2.6.1 -q           # to load the Common Voice dataset from Hugging Face\n",
        "!pip install transformers -q              # to load transformer models from Hugging Face\n",
        "!pip install librosa -q                   # to pre-process audio files\n",
        "!pip install evaluate>=0.30 -q            # to assess the performance of the model\n",
        "!pip install jiwer -q                     # to assess the performance of the model\n",
        "!pip install gradio -q                    # to create demo interfaces\n",
        "!export LC_ALL=en_US.UTF-8                # needed in order to pip install googletrans below\n",
        "!pip install googletrans==4.0.0-rc1 -q    # to create a google translate API"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging into Hugging Face so we can upload model checkpoints while training and upload the trained model\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lQonteAEJvxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "DUaftc51KG0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Common Voice data from Hugging Face\n",
        "# The model will be trained on Japanese, i.e. \"ja\"\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ja\", split=\"train\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"ja\", split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "cLAhmYQ2KJGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seen on Hugging Face, the dataset contains multiple columns with additional information. We remove all columns except for the audiofile and transcribed text for fine-tuning\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "y82GhfRaK9HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "y_GVuKTaLpcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline for Automatic Speech Recognition can be divided into three stages:\n",
        "- Pre-processing, feature extraction on audio inputs\n",
        "- Sequence-2-Sequence mapping\n",
        "- Post-processing, tokenization of model output to text format\n",
        "\n",
        "Whisper also provides the feature extractor and tokenizer which can be loaded from Hugging Face"
      ],
      "metadata": {
        "id": "aJZaNNG2L23j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Processing Tools"
      ],
      "metadata": {
        "id": "TXtwiuc8OSLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the WhisperFeatureExtractor\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
      ],
      "metadata": {
        "id": "5T8JtZnUL2ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the WhisperTokenizer setting the language to Japanese and the task to transcribe\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Japanese\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "6RLUcu-gLqUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading WhisperProcessor to wrap both WhisperFeatureExtractor and WhisperTokenizer into a single class\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Japanese\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "IaujlgzfNojs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "WvkC1DMqOj3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the form of the data\n",
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "RlsDSjr6Ok21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seen above, the input is sampled at 48kHz. The WhisperFeatureExtractor requires a sampling rate of 16kHz, i.e. the data has to be resampled by using Audio from Hugging Face Datasets\n",
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "IoZHHmrYOzG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking that the data has been resampeld to 16 kHz\n",
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "K3gTHPL9PdVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function for data pre-processing, which resamples the data, extracts the features and tokenizes the transcriptions\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "RtcVdOuePoL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing the training dataset using the function above\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "uNVsKYCqQLx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Lb-QicfiQcvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained by using ðŸ¤— Trainer. To do so we have to:\n",
        "\n",
        "- Define a data collator: the data collator takes pre-processed data and prepares PyTorch tensors\n",
        "\n",
        "- Evaluation metrics: we evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric.\n",
        "\n",
        "- Load a pre-trained checkpoint: we load a pre-trained checkpoint and configure it for training.\n",
        "\n",
        "- Define the training configuration: this is used by the ðŸ¤— Trainer to define the training schedule."
      ],
      "metadata": {
        "id": "b5Aex-RiQoZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator"
      ],
      "metadata": {
        "id": "SnRZoammRsJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data collator for a sequence-to-sequence speech model is unique in the sense that it \n",
        "treats the `input_features` and `labels` independently: the  `input_features` must be \n",
        "handled by the feature extractor and the `labels` by the tokenizer.\n",
        "\n",
        "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram \n",
        "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
        "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
        "\n",
        "The `labels` on the other hand are un-padded. We first pad the sequences\n",
        "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens \n",
        "are then replaced by `-100` so that these tokens are **not** taken into account when \n",
        "computing the loss. We then cut the BOS token from the start of the label sequence as we \n",
        "append it later during training.\n",
        "\n",
        "We can leverage the `WhisperProcessor` we defined earlier to perform both the \n",
        "feature extractor and the tokenizer operations:"
      ],
      "metadata": {
        "id": "WU7RNU3qSCxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a class for the data collator\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "RQAMREfLQddc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "NwrkdO0fSGxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "azbKRgWMQeaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then simply have to define a function that takes our model \n",
        "predictions and returns the WER metric. This function, called\n",
        "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
        "in the `label_ids` (undoing the step we applied in the \n",
        "data collator to ignore padded tokens correctly in the loss).\n",
        "It then decodes the predicted and label ids to strings. Finally,\n",
        "it computes the WER between the predictions and reference labels:"
      ],
      "metadata": {
        "id": "EYIFp1OfSjEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the evaluation metric \"WER\"\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "3IHygR5mQfI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function that returns the WER based on model predictions\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "i3h9o_oiSbTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "mHLxR9LVSlkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pre-trained checkpoints from the tiny Whisper model\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
      ],
      "metadata": {
        "id": "7wgMukzeSmfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overriding generation arguments, not tokens are forced as decoder outputs or suppresed during generation\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "GkChwsThS2Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "UlPoEouBTAxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE check how many rows of data and adjust warm-up and max steps"
      ],
      "metadata": {
        "id": "pdOpqAZiTj4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameters for training referring to the Seq2SeqTrainingArguments docs.\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "#training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./Japanese_Fine_Tuned_Whisper_Model\",\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "kt6z3q2oTB-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forwarding the training arguments, model, dataset, data collator and evaluation metric function to the ðŸ¤— Trainer\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "el9J8NSUTxEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the processor object\n",
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "OTYXYrgPUMXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "wbWKw57PURmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "krG2UKN7Uajg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pushing the model to Hub"
      ],
      "metadata": {
        "id": "fSekRQK9TwN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub('Japanese_Fine_Tuned_Whisper_Model')"
      ],
      "metadata": {
        "id": "pZ2jaosrUh_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub('Japanese_Fine_Tuned_Whisper_Model')"
      ],
      "metadata": {
        "id": "JCIn4QhpUnqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",\n",
        "    \"dataset_args\": \"config: ja, split: test\",\n",
        "    \"language\": \"ja\",\n",
        "    \"model_name\": \"Japanese_Fine_Tuned_Whisper_Model\",\n",
        "    \"finetuned_from\": \"openai/whisper-tiny\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "IV54GsXqUpc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "t19xTvRWU0oB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper Gradio Demo"
      ],
      "metadata": {
        "id": "9Ozaf84oU5DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"NadiaHolmlund/Japanese_Fine_Tuned_Whisper_Model\")  # change to \"your-username/the-name-you-picked\"\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface_transcribe = gr.Interface(\n",
        "    fn=transcribe, \n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Tiny Japanese\",\n",
        "    description=\"Realtime demo for Japanese speech recognition using a fine-tuned Whisper tiny model.\",\n",
        ")\n",
        "\n",
        "iface_transcribe.launch()"
      ],
      "metadata": {
        "id": "PtlYLyfYU1bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Translate API Demo"
      ],
      "metadata": {
        "id": "k-EnHharVBXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    result = translator.translate(text, dest='en')\n",
        "    translation = result.text\n",
        "    pronunciation = translator.translate(translation, dest='ja').pronunciation\n",
        "    return f\"Pronunciation: {pronunciation}\", f\"Translation: {translation}\"\n",
        "\n",
        "iface_translate = gr.Interface(\n",
        "    fn=translate,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"text\", \"text\"],\n",
        "    output_labels=[\"Pronunciation\", \"Translation\"],\n",
        "    title=\"Google Translate\"\n",
        ")\n",
        "\n",
        "iface_translate.launch()"
      ],
      "metadata": {
        "id": "NbF6v4zwVC_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper + Google Translate API Demo"
      ],
      "metadata": {
        "id": "y0NFidyEVGPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"NadiaHolmlund/Fined_Tuned_Whisper_Model\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    transcription = pipe(audio)[\"text\"]\n",
        "    result = translator.translate(text, dest='en')\n",
        "    translation = result.text\n",
        "    pronunciation = translator.translate(translation, dest='ja').pronunciation\n",
        "    return f\"Transcription: {transcription}\", f\"Pronunciation: {pronunciation}\", f\"Translation: {translation}\"\n",
        "\n",
        "iface_transcribe = gr.Interface(\n",
        "    fn=transcribe, \n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"), \n",
        "    outputs=[\"text\", \"text\", \"text\"],\n",
        "    output_labels=[\"Transcription\", \"Pronunciation\", \"Translation\"],\n",
        "    title=\"Whisper Tiny Japanese\",\n",
        "    description=\"Realtime demo for Japanese speech recognition using a fine-tuned Whisper tiny model.\",\n",
        ")\n",
        "\n",
        "iface_transcribe.launch()"
      ],
      "metadata": {
        "id": "D1l3rntaVIis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}